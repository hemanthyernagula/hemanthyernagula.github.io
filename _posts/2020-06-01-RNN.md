---
layout: post
title: Recurrent Neural Networks
categories: article
permalink: /:categories/:title
tags: [Sequential, DeepLearning, Rnn]
progress: True
image: '/assets/images/RNN_example.svg'
---

**RNN** is popularly used to train on sequential data, sequential may be a _sequence of words_ i.e sentences, sequential may be _time series_ data or sequential may _sequence of pixels_.

# Problem with MLP for sequential data:

Suppose we have a data-set having total number of words of 1000 and lets a pick first data point i.e  **This article looks good**.

Now this sentence must be featurized, lets featurize into _one hot encoding_, we'll be having a vector of size 1000(since total number of words is 1000) for each word.



![RNN Ex](/assets/images/RNN_example.svg)

We shall pass this vectors  to an **MLP**, note that our first input(our ex) is having four vectors each of size 1k and only this four vectors will be input to the **MLP**, so the total size for input layer will be 1k x 4 -> 4k

![RNN Ex](/assets/images/mlp for rnn.svg)

Ok we have build a MLP for our example sentence but, will the size of input be the same for all the sentences? no, right? sizes of the input sentences will vary sentence so how can we fix this?  One way is using padding i.e fixing size of input layer with maximum size of the sentences in our data-set. Suppose let's think maximum length of sentence is 10 so our input layer size will be 10 x 1k i.e 10k. For each input this 10k size vector is filled with vectors of each sentence and remaining will be filled with **zero**. But assume that next layer is having 20 activation units so our trainable weights will be **10k(input layer) X 20** i.e 20k which is very huge vector, in real world problems it is common to have data-set having millions of words upon which training such MLP becomes horrible. 

# Applications:

1. **Sentiment analysis:** Given a sequence of words model predicting weather it is a positive or negative sentence.
2. **Time Series Analysis:** Given the data at  each time stamp model predicting what will be the value at next _time stamp_.
3. **Language Translator:** Given sequence information in _language A_ model predicting sequence of information in _language B_.
4. **Image Captioning:** Given an image model predicting sequence of words that forms as caption for that image.
5. **Speech Recognition:** Given an sequence format of an audio model predicting sequence of words so that audio is transformed into text data.



# References:

1. https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7
2. https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
3. [math](https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78)
4. http://www.deeplearningbook.org/contents/rnn.html
5. https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt